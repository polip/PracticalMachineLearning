---
title: "Practical Machine Learning Course Project"
author: "Ivan Lozo"
date: '15 svibnja 2017 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

### Introduction

This is an Markdown document created with purpose to present results of predictive analyses on people personal activity. It is  created as result of Practical Machine Learning Course Project.
Data is provided by http://groupware.les.inf.puc-rio.br/har.
Purpose of the modelling is to predict the manner in which people did the exercise which is part of the "classe" variable in the training set.

### Importing data

For importing data we will use Hedley's readr package.

```{r}
library(readr)

training <- read_csv("~/R/PracticalMachineLearning1/pml-training.csv", 
col_types = cols(kurtosis_picth_arm = col_double(), 
       kurtosis_picth_belt = col_double(), 
       kurtosis_yaw_arm = col_double(), 
       kurtosis_yaw_belt = col_double(), 
       kurtosis_yaw_dumbbell = col_double(), 
        kurtosis_yaw_forearm = col_double(), 
        skewness_pitch_arm = col_double(), 
        skewness_roll_belt.1 = col_double(), 
        skewness_yaw_arm = col_double(), 
        skewness_yaw_belt = col_double(),
       skewness_yaw_dumbbell = col_double(),
       skewness_yaw_forearm = col_double()))
        

testing <- read.csv("pml-testing.csv")
```

### Data inspection 

Let's examine training data set that we will use for modelling.

Training dataset contains 19 622 rows and 160 columns
```{r}
dim(training)
```


```{r}

head(training,n = 20)
```
Let's look at the distribution of target variable.
```{r}
table(training$classe)
```


First, we will remove also first 6 columns, since they don't contain usefull data.

```{r}
training <- training[, -(1:6)]
testing <- testing[, -(1:6)]
```

Then, we will remove columns with all missing variables.

```{r}
allmisscols <- apply(training,2, function(x)all(is.na(x)))
colswithallmiss <-names(allmisscols[allmisscols>0])

training <- training[, -which(names(training)%in%colswithallmiss)]
testing <- testing[, -which(names(testing)%in%colswithallmiss)]
```

We will also remove variables with NAs share above 90%.
```{r}
training <- training[,colMeans(is.na(training))<0.10]
testing <- testing[,colMeans(is.na(testing))<0.10]

```

For the variables with lower than 90% of NAs we will impute mean value.

```{r}

training[] <- lapply(training,function(x) ifelse(is.na(x), mean(x,na.rm = T),x))
```

Now that we have clean data with missing values, we will examine corellation range between variables.

```{r}
library(corrplot)

trainingCor <- cor(training[,-54])
corrplot(trainingCor,order = "hclust")

```

Finally, we will remove variables correlated more than 80%.
```{r, message=FALSE, warning=FALSE}
library(caret)
highlyCorr <- findCorrelation(trainingCor,cutoff = 0.80)

training <- training[,-c(highlyCorr)]
testing <- testing[,-highlyCorr]
```

After cleaning data and removing highly corellated variables we have dataset of 40 predictors left on which we will train gbm algorithm.

```{r}
dim(training)
```


Now, we will fit gbm model from caret package in order to build a model for prediction of classe variable. First, we will transform target variable to factor, then divide dataset to train (75%) and validation (25%). Next, we will transform predictors using scale and center  mehods
```{r}
training$classe <- factor(training$classe)
set.seed(1)
trainID <- createDataPartition(training$classe,p = 0.75, list=FALSE)

train <- training[trainID,]
validation <- training[-trainID,]

#remove original training dataset
rm(training)

trainPreProcess <- preProcess(x = train[,-41], method = c("center","scale"))

trainTransform <- predict(trainPreProcess,train)

#remove original training
rm(train)


```
We will use crossvalidation resampling with 5 folds in order to decrease computation time. 

Fit the gbm model.
```{r, echo=TRUE, message=FALSE, warning=FALSE}

trainCntr <- trainControl(method = "cv",number = 5)
gbm <- train(classe~.,data = trainTransform, method="gbm",verbose = FALSE,trControl = trainCntr)
```


Now, we will transform validation set and make prediction in order to calculate accuracy and out of sample error.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
validTransform <- predict(trainPreProcess,validation)
valPred <- predict(gbm,validTransform)

accuracy <- sum(valPred==validTransform$classe) / length(valPred)
outError <- 1 - accuracy
confusionMatrix(valPred,validTransform$classe)
```


Accuracy on the validation data is `r round(accuracy,2)`, which means that out of sample error is `r round(outError,2) `


Finally, we will transform test dataset and make prediction for the final submission.

```{r, echo=TRUE}
testTransform <- predict(trainPreProcess,testing)
testPred <- predict(gbm,testTransform)
testPred
```




